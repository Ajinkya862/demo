import asyncio
import streamlit as st
from streamlit_webrtc import VideoTransformerBase, webrtc_streamer
import av
import mediapipe as mp
import cv2
# import TSCAN

TF_ENABLE_ONEDNN_OPTS=0

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        super().__init__()
        self.counter = 0
        self.frame_list = []
        #self.face_preprocessing = FacePreprocessing()

    async def recv(self, frame):
        if self.counter > 90:
            self.counter = 0
            self.frame_list = []

        img = frame.to_ndarray(format="bgr24")
        self.frame_list.append(img)
        self.counter += 1 

        # Pass the frame_list to face_preprocessing after every 90 frames
        # if self.counter % 90 == 0:
        #     await self.async_face_preprocessing()

        return img

    async def async_face_preprocessing(self):
        processed_frames = await asyncio.to_thread(self.face_preprocessing.process_frames, self.frame_list)
        # Now, processed_frames contains the preprocessed faces, you can further use or display them as needed
        st.write("Processed Frames:", len(processed_frames))

class FacePreprocessing:
    def __init__(self):
        self.mp_face_detection = mp.solutions.face_detection1
        self.face_detection = self.mp_face_detection.FaceDetection()
        self.face_size = (72, 72)

    def process_frames(self, frame_list):
        processed_frames = []
        for frame in frame_list:
            # Use mediapipe for face detection
            result = self.face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

            if result.detections:
                # Get the bounding box of the face
                bboxC = result.detections[0].location_data.relative_bounding_box
                ih, iw, _ = frame.shape
                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)

                # Crop the face
                face = frame[y:y + h, x:x + w]

                # Resize the face frames
                resized_face = cv2.resize(face, self.face_size, interpolation=cv2.INTER_AREA)

                # Append the processed face to the list
                processed_frames.append(resized_face)

        return processed_frames
    
# def chunk(self, frames,  chunk_length=210):
#     """Chunk the data into small chunks.

#     Args:
#         frames(np.array): video frames.
#         bvps(np.array): blood volumne pulse (PPG) labels.
#         chunk_length(int): the length of each chunk.
#     Returns:
#         frames_clips: all chunks of face cropped frames
#         bvp_clips: all chunks of bvp frames
#     """
#     #  chunk length vale batch banav if 205 frames 10 CL then  20 batch and 5 will be ignored
#     clip_num = frames.shape[0] // chunk_length
#     frames_clips = [frames[i * chunk_length:(i + 1) * chunk_length] for i in range(clip_num)]
#     #bvps_clips = [bvps[i * chunk_length:(i + 1) * chunk_length] for i in range(clip_num)]
#     return np.array(frames_clips)

# def process(self, frames):
#     data=list()
#     f_c = processed_frames.copy()   
#     def diff_normalize_data(data):
#         """Calculate discrete difference in video data along the time-axis and nornamize by its standard deviation."""
#         n, h, w, c = data.shape
#         diffnormalized_len = n - 1
#         diffnormalized_data = np.zeros((diffnormalized_len, h, w, c), dtype=np.float32)
#         diffnormalized_data_padding = np.zeros((1, h, w, c), dtype=np.float32)
#         for j in range(diffnormalized_len):
#             diffnormalized_data[j, :, :, :] = (data[j + 1, :, :, :] - data[j, :, :, :]) / (
#                     data[j + 1, :, :, :] + data[j, :, :, :] + 1e-7)
#         diffnormalized_data = diffnormalized_data / np.std(diffnormalized_data)
#         diffnormalized_data = np.append(diffnormalized_data, diffnormalized_data_padding, axis=0)
#         diffnormalized_data[np.isnan(diffnormalized_data)] = 0
#         return diffnormalized_data
#     data = np.concatenate(data, axis=-1)
#     frames_clips = chunk(data)
#     return frames_clips




# def prediction(self):
#     data_loader=[]
#     data_loader=self.process(frames)
#     self.model = TSCAN(frame_depth=self.frame_depth, img_size=config.TEST.DATA.PREPROCESS.RESIZE.H).to(self.device)
#     self.model = torch.nn.DataParallel(self.model, device_ids=list(range(1)))
#     path='C:/Users/CHANDRASHEKHAR/Desktop/latest/PURE_TSCAN.pth'
#     self.model.load_state_dict(torch.load(path,map_location=torch.device('cpu')))
#     print("Testing uses pretrained model!")
#     self.model = self.model.to(self.config.DEVICE)
#     self.model.eval()
#     print("Running model evaluation on the testing dataset!")
#     with torch.no_grad():
#         batch_size = data_loader[0].shape[0]
#         data_test=test_batch[0].to(
#                     self.config.DEVICE)
    
#     pass 
    
    
# Define the Streamlit app
def main():
    st.title("WebRTC Video Stream")

    # Configure WebRTC streamer with custom transformer
    webrtc_ctx = webrtc_streamer(
        key="example",
        video_processor_factory=VideoTransformer,
        async_processing=True,  # Use async_processing instead of async_transform
    )

if __name__ == "__main__":
    main()
